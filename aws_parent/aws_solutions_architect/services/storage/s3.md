* blob/object storage
* unlimited space
* pay as you go
* static content
* access using a key
* not file system, mountable
* not good:
    * to have lots of small files 
    * if need a file system
    * no search
    * not good for rapidly changing data
* 11 9's durability
* no indexing so CAN'T search for an object in S3 bucket
  * index objects in S3

#### Storage classes (also known as tiers)
* all classes have a base charge per volume of data stored
    * additional charges include retrieval, minium duration, minimum object, etc. 
    * base charge adjusts based on durability and frequency of access   
        * glacier base charge is extremely cheap but other charges add up when trying to retrieve
        * same with IA
        * one zone trades off AVAILABILITY for cheaper base charge  
            * note this is different from EFS one zone which trades off durability

* all have 11 9's durability
* all exist in >- 3 AZ's
    * except ONE ZONE classes which exists in one zone
* all have 99.9% availability 
    * except One Zone which has 99.5$

* Standard and Intelligent Tiering have no retrieval charge to get the item, all others have a retrieval charge

* Lifecycle Rules
    * transition objects between tiers automatically (or delete)
    * If you want to automatically move objects to Glacier after a certain time:


* Minimum Storage Duration Charge
    * AWS charges you as if the object is stored for a minimum of 180 days, even if you delete it earlier.

* Minimum Billable Object Size 
    * file smaller than this will still get charged for this size
    * If you store a file of 10 KB in Glacier Deep Archive, you will still be billed as if it were 40 KB.

* Cost to retrieve items
    * This fee is separate from the standard S3 request charges 
    * AWS prorates retrieval fees  based on the exact amount of data you retrieve,
        * i.e. if you retrieve 10MB will be charged based on a fraction of the 1GB price


* Standard:
    * S3 Standard: General-purpose storage for frequently accessed data.
    * Minimum Storage Duration Charge: None
    * Minimum Billable Object Size: None
    * Retrieval Fee: None

* Intelligent-Tiering:
  * Minimum Storage Duration Charge: None
  * Minimum Billable Object Size: None
  * Retrieval Fee: None

* Standard-IA(infrequent access):
  * Minimum Storage Duration Charge: 30 Days
  * Minimum Billable Object Size: 128 KB

* One Zone-IA(infrequent access):
  * Minimum Storage Duration Charge: 30 Days
  * Minimum Billable Object Size: 128 KB

* Glacier Instant Retrieval:
  * Minimum Storage Duration Charge: 90 Days
  * Minimum Billable Object Size: 128 KB

* Glacier Flexible Retrieval:
  * Minimum Storage Duration Charge: 90 Days
  * Minimum Billable Object Size: 40 KB

* Glacier Deep Archive:
  * Minimum Storage Duration Charge: 180 Dayskeep in mind 
  * Minimum Billable Object Size: 40 KB


#### Replication
* cross region replication (CRR)
* same region replication (SRR)
* designed to be used for backup and disaster

* can combine with lifecycle rules to transfer to different tiers

* Replication Time Control (RTC)
  * most objects replicated within seconds
  * 99.99% withing 15 minutes
  * get alarm if one is beyong 15 minute
  * helpful for compliance rules


#### S3 event notification
* s3 object: create/remove/restore/replicate...etc.
* for example: trigger an even when object uploaded and create a thumbnail from it
* these notifications typically delivered within seconds but can take up a minute or longer sometimes
* can trigger: SNS, SQS, Lambda, Amazon Event Bridge
* Event bridge
  * Once the events sent to Event Bridge can set up complex AWS logic to send to a bunch of AWS services
  * sends more meta-data about the s3 object to Event Bridge than other services
  * can set up multiple desinations for a single rule
  * have event functions: archive, replay
  * reliable delivery



#### Performance
* latency 100-200 ms
* application can achieve at least 3,500 POSTS/COPY/PUT/DELETE or 5,500 GET/HEAD per second per prefix in a bucket
* no limit to how many prefixes can have in a bucket
* the prefix is essentially the path of the file without the bucket nae or filename
  * /my-bucket/x/y/z/myfile.txt
  * the prefix would be /x/y/z/
* can create different prefixes for the file to scale more http requests
  * i.e. if spread reads across 4 prefixes can have up to 22,000 requests per second

* multi-part upload 
  * reccomended for files >100MB
  * required for files >100GB
  * parallelize the file upload for speed
  * note: if you stop the multi-part upload early and leave file upload incomplete there will be straggler data in S3 
    * set up a lifecycle policy to abort and delete the multipart upload stragglers after X number of days

* S3 transfer accelleration
  * client can transfer file to edge location which will then internally transfer file to s3 bucket
  * compatible with multi part upload so can combine to bump speed

* S3 byte range fetches
  * parlellize GETS by requesting specific byte ranges
  * basically chunks up the file and pulls all the chunks at once in parallel
  * better resilience in case of failures
  * can be used to speed up downloads
  * can also allow to retrieve parts of files i.e. submit a byte range, can save bandwidth costs

#### S3 select and Glacier select
* can use simple SQL statements to query CSV, JSON, and Parquet files
* store file in s3
* use S3 select API to query the file with a SQL statement
* this saves transfer bandwidth as it only transfers data you need


#### Storage Class Analysis
* feature of s3 that helps optimize storage costs by analyzing objects and determining the best storage class
* creates a CSV report
* report updated daily
* takes a day or two for it to start working after enabled
* can also visualize in Amazon Quick Sight
* helps to build lifecycle rules


#### Storage Lens
* Amazon S3 Storage Lens is an analytics tool that provides comprehensive insights into the storage usage and activity across all your Amazon S3 buckets.
* Provides over 60 metrics to analyze storage usage and activity.
* Aggregates data across multiple accounts, buckets, and regions into a single view.
* Offers a default user-friendly console dashboard, but can also customize your own
* Automatically generates actionable recommendations for optimizing costs and performance.
* Export metrics to Amazon S3 for further analysis with tools like Amazon QuickSight or third-party solutions.(JSON or Parquet)
* two tiers:
  * Free Metrics: Basic metrics are included at no additional cost.
    * storage only 14 days
    * 28 different metrics
  * Advanced Metrics and Recommendations: Paid tier with additional insights, activity metrics, and advanced recommendations.
    * storage is 15 months
    * collects metrics at the prefix level
    * can access metrics in cloudwatch free
    * add more metrics
* metrics
  * bytes and object count -> helps identify fastest growing or not used buckets and prefixes
  * cost optimization metrics
    * NonCurrentVersionStorageBytes - old versions unuusued
    * IncompleteMultipartUploadStorageBytes -  incomplete uploads not needed
    * can write life cycle 
  * data protection metrics
    * versioning enabled?
    * MFA required for delete?
    * crossregion replication rule count
    * identify buckets not following data protection best practices
  * access managment metrics - ownership related info
  * event metrics - events fired, enabled
  * performance metrics - s3 transfer acceleration
  * activity metrics - all requests/byt downloaded
  * status code metrics - 200's, forbiddens, notFound's

#### Static website hosting
* S3 can be enabled as a static website with enable static hosting config


#### Signed URL
* can create a temprary time limited url for access to files
* the url is created with permissions as the person who created the presigned url